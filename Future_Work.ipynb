{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_ngrams(input_list, n):\n",
    "    return zip(*[input_list[i:] for i in range(n)])\n",
    "\n",
    "word_dict = find_ngrams(text_content,3)\n",
    "\n",
    "#for i in range(listLen):\n",
    "#    word_dict['_'.join(scoredList[i][0])] = scoredList[i][1]\n",
    "\n",
    "wordCloud.generate(\" \".join(word_dict)) \n",
    "plt.title('Most frequently occurring trigrams connected with an underscore_')\n",
    "plt.imshow(wordCloud)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unsupervised Learning: As only the job titles are available, I am trying to cluster the titles based on string similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.metrics.distance import jaccard_distance\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "])        \n",
    "X = pipeline.fit_transform(clean_jobtitle_list).todense()\n",
    "\n",
    "pca = PCA(n_components=5).fit(X)\n",
    "data2D = pca.transform(X)\n",
    "plt.scatter(data2D[:,0], data2D[:,1])\n",
    "\n",
    "kmeans = KMeans(n_clusters=5).fit(X)\n",
    "centers2D = pca.transform(kmeans.cluster_centers_)\n",
    "\n",
    "plt.hold(True)\n",
    "plt.scatter(centers2D[:,0], centers2D[:,1], \n",
    "            marker='x', s=200, linewidths=3, c='r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "X = vectorizer.fit_transform(clean_jobtitle_list)\n",
    "\n",
    "#print(X)\n",
    "true_k = 10\n",
    "model = KMeans(n_clusters=true_k, init='k-means++', max_iter=100, n_init=1)\n",
    "model.fit(X)\n",
    "print(\"Top terms per cluster:\")\n",
    "order_centroids = model.cluster_centers_.argsort()[:, ::-1]\n",
    "terms = vectorizer.get_feature_names()\n",
    "for i in range(true_k):\n",
    "    print (\"Cluster %d:\" % i)\n",
    "    for ind in order_centroids[i, :10]:\n",
    "        print (terms[ind])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec based clustering and function recommendation\n",
    "### need labelled dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Load Google's pre-trained Word2Vec model.\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format('./GoogleNews-vectors-negative300.bin', binary=True)  \n",
    "\n",
    "\n",
    "\n",
    "def avg_sentence(sentence, wv):\n",
    "    v = np.zeros(300)\n",
    "    for w in sentence:\n",
    "        if w in wv:\n",
    "            v += wv[w]\n",
    "    return v / len(sentence)\n",
    "\n",
    "def cosine_sim(a, b):\n",
    "    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
    "\n",
    "\n",
    "\n",
    "with open('First100.csv') as csvfile:\n",
    "    rows = csv.reader(csvfile)\n",
    "    res = list(zip(rows))\n",
    "\n",
    "data=res[1:-1]\n",
    "data\n",
    "\n",
    "inputv = avg_sentence(\"Marketing Partner\".split(), model.wv)\n",
    "avgs = list(map(lambda p: p + (avg_sentence(p[0][0].split(), model.wv),), data))\n",
    "sims = list(map(lambda p: p[0][:2] + (cosine_sim(inputv, p[0][2]),), avgs))\n",
    "\n",
    "most_similar = sorted(sims, key=lambda p: p[2], reverse=True)\n",
    "\n",
    "print(most_similar)\n",
    "\n",
    "#title_list=[list(chain.from_iterable(clean_title)) for clean_title in clean_jobtitle_list]\n",
    "#print(title_list[:100])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
